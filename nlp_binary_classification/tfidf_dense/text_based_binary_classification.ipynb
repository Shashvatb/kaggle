{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport os\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv', encoding='utf-8').fillna('')\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns\n# df = df.drop(['id'], axis=1)\nprint(df.head())\nprint(list(set(df['location'].tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleaning(df, keyword_col, text_col, input_col, location_col):\n    df[input_col] = df[text_col] + ' ' + df[keyword_col]# + ' ' + df[location_col]\n    df[input_col] = df[input_col].apply(lambda x: str(x).lower())\n    df[input_col] = ' ' + df[input_col] + ' '\n    df[input_col] = df[input_col].replace(r'http.*\\s', ' ', regex=True)\n    df[input_col] = df[input_col].replace(r'\\%20', ' ', regex=True)\n    \n    df[input_col] = df[input_col].replace(r'\\\\x.*\\s', '', regex=True)\n    df[input_col] = df[input_col].replace(r'\\n', ' ', regex=True)\n    df[input_col] = df[input_col].replace(r'\\.|\\'', '', regex=True)\n    df[input_col] = df[input_col].replace(r'\\.|\\#|\\-|\\!|\\?|\\;|\\)|\\(|\\:|\\@|\\'|\\[|\\]|\\&|\\||\\/', ' ', regex=True)\n    stop = [i.lower() for i in stopwords.words('english')]\n    df['temp'] = df[input_col].apply(lambda x: str(x).split())\n    df['temp'] = df['temp'].apply(lambda x: [i for i in x if i not in stop])\n    df[input_col] = df['temp'].apply(lambda x: ' '.join(x))\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = cleaning(df, 'keyword', 'text', 'input','location')\ndf = df[['target', 'input']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df = 2, ngram_range=[1,2])\nvectorizer.fit(df['input'])\nvocab_length = len(vectorizer.get_feature_names())\nvocab_length\n# msk = np.random.rand(len(df)) < 0.8\n# df_train = df[msk]\n# df_test = df[~msk]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class textData(Dataset):\n    def __init__(self, df, vectorizer):\n        self.x = torch.from_numpy(vectorizer.transform(df['input'].values).astype(np.float32).todense())\n        self.y = torch.from_numpy(df['target'].values.astype(np.float32))\n        self.m = self.x.shape[0]\n    \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        return self.m\n    \ndataset_train = textData(df, vectorizer)\ndataloader_train = DataLoader(dataset=dataset_train, batch_size=32, shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda:0')\n    print('cuda')\nelse:\n    device = torch.device('cpu')\n    print('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, vocab_length):\n        super().__init__()        \n        self.fc1 = nn.Linear(vocab_length, 16)\n        self.fc2 = nn.Linear(16, 32)\n        self.fc3 = nn.Linear(32, 64)\n        self.fc4 = nn.Linear(64, 128)\n        self.dropout = nn.Dropout(0.7)\n#         self.normalize = nn.BatchNorm1d()\n        self.fc5 = nn.Linear(128, 1024)\n\n        self.fc8 = nn.Linear(1024, 1)\n    \n    \n    def forward(self, x):      \n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = self.dropout(x)\n#         x = self.normalize(x)\n        x = F.relu(self.fc5(x))\n        x = self.dropout(x)\n#         x = self.normalize(x)\n        x = torch.sigmoid(self.fc8(x))\n        \n        return x\n    \n\nmodel = Net(vocab_length).to(device)\noptimizer = optim.Adam(model.parameters())\nloss_function = nn.BCEWithLogitsLoss()\nEPOCHS = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(dataloader, model, loss_function, optimizer, EPOCHS, vocab_size):\n    losses = []\n    for epoch in range(EPOCHS):\n        epoch_losses = []\n        for i, (inputs, labels) in tqdm(enumerate(dataloader)):\n            inputs = inputs.to(device)\n            labels = labels.view(-1,1).to(device)    \n            model.zero_grad()\n            optimizer.zero_grad()\n            y_hat = model.forward(inputs)\n            loss = loss_function(y_hat, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_losses.append(loss)\n            if 32 % 16 ==0:\n                losses.append(loss)\n        print(np.min(epoch_losses))\n    return model, losses    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(model, dataloader, vocab_size):\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in tqdm(enumerate(dataloader)):\n            real_class = labels.view(-1, 1).to(device)\n            net_out = model(inputs.view(-1,vocab_size).to(device))\n            predicted_class = torch.round(net_out)\n            correct +=  torch.sum(predicted_class == real_class).item()\n            total += inputs.shape[0]\n    print(correct/total)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model, losses = train(dataloader_train, model, loss_function, optimizer, EPOCHS, vocab_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validate(model, dataloader_train, vocab_length)\n# validate(model, dataloader_test, vocab_length)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_orig = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nprint(df_test_orig.columns)\ndf_test = df_test_orig.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = cleaning(df_test, 'keyword', 'text', 'input', 'location')\ndf_test = df_test[[ 'input']]\ndf_test['target'] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test = textData(df_test, vectorizer)\ndataloader_test = DataLoader(dataset=dataset_test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, dataloader, vocab_size):\n    Y = []\n    with torch.no_grad():\n        for i, (inputs, labels) in tqdm(enumerate(dataloader)):\n            net_out = model(inputs.view(-1,vocab_size).to(device))\n            predicted_class = torch.round(net_out).view(-1).detach().cpu().numpy()\n            Y += list(predicted_class)\n    return Y        \nY = test(model, dataloader_test, vocab_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv').fillna('')\ndf_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = [int(i) for i in Y]\nY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_test_orig.columns)\ndf_test_orig['target'] = Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_test_orig.head())\ndf_test_orig = df_test_orig[['id', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_orig.to_csv('/kaggle/working/result.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}